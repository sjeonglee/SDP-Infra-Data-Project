{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBg-t2b2QOP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "1377dae7-95c0-43f5-9356-31024cefed7c"
      },
      "source": [
        "# Reference tutorial: https://www.thepythoncode.com/article/text-classification-using-tensorflow-2-and-keras-in-python\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du6rk5hBRkCR"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from glob import glob\n",
        "import random\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3asdBhjdTrUU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "16564fc9-83aa-4c53-f347-cda849b9be04"
      },
      "source": [
        "dataFrame = pd.read_csv(\"gdrive/My Drive/2020_SDP/Project_MaPPIng_Phase II/Tech_Team/policy_dataset.csv\", encoding='UTF-8')\n",
        "dataFrame.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>url</th>\n",
              "      <th>body</th>\n",
              "      <th>title_google</th>\n",
              "      <th>main_query</th>\n",
              "      <th>aux_query</th>\n",
              "      <th>validation_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5ec0e7c54e24cd5aeeb9ae69</td>\n",
              "      <td>https://www.daily-sun.com/post/474802/Corona-h...</td>\n",
              "      <td>The under-construction 660MW coal-fired power ...</td>\n",
              "      <td>title</td>\n",
              "      <td>\"\"\"\"\"</td>\n",
              "      <td>[\"covid\",\"Corona\",\"Coronavirus\",\"COVID-19\"]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5ec0e7cc4e24cd5aeeb9ae6a</td>\n",
              "      <td>https://www.newagebd.net/article/102134/payra-...</td>\n",
              "      <td>The country’s largest Payra 1320 MW Thermal Po...</td>\n",
              "      <td>title</td>\n",
              "      <td>\"\"\"\"\"</td>\n",
              "      <td>[\"covid\",\"Corona\",\"Coronavirus\",\"COVID-19\"]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5ec0e7cc4e24cd5aeeb9ae6b</td>\n",
              "      <td>https://www.airport-technology.com/news/covid-...</td>\n",
              "      <td>Visit our Covid-19 microsite for the latest co...</td>\n",
              "      <td>title</td>\n",
              "      <td>\"\"\"\"\"</td>\n",
              "      <td>[\"covid\",\"Corona\",\"Coronavirus\",\"COVID-19\"]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5ec0e7cd4e24cd5aeeb9ae6c</td>\n",
              "      <td>https://global.chinadaily.com.cn/a/202003/26/W...</td>\n",
              "      <td>By David Ho in Hong Kong | China Daily Global ...</td>\n",
              "      <td>title</td>\n",
              "      <td>\"\"\"\"\"</td>\n",
              "      <td>[\"covid\",\"Corona\",\"Coronavirus\",\"COVID-19\"]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5ec0e7ce4e24cd5aeeb9ae6d</td>\n",
              "      <td>http://www.fredericton.ca/en/news/city-hall/ci...</td>\n",
              "      <td>Fredericton City Council has decided to postpo...</td>\n",
              "      <td>title</td>\n",
              "      <td>\"\"\"\"\"</td>\n",
              "      <td>[\"covid\",\"Corona\",\"Coronavirus\",\"COVID-19\"]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        _id  ... validation_flag\n",
              "0  5ec0e7c54e24cd5aeeb9ae69  ...               1\n",
              "1  5ec0e7cc4e24cd5aeeb9ae6a  ...               1\n",
              "2  5ec0e7cc4e24cd5aeeb9ae6b  ...               1\n",
              "3  5ec0e7cd4e24cd5aeeb9ae6c  ...               1\n",
              "4  5ec0e7ce4e24cd5aeeb9ae6d  ...               1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmjX-tswuXv_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "1d49584a-c4a0-4321-9ec3-24f124cc6deb"
      },
      "source": [
        "train_set = pd.read_csv(\"gdrive/My Drive/SJ/data/train_set.tsv\", sep=\"\\t\", encoding='UTF-8', names=['original_text', 'validation_flag', 'body'])\n",
        "train_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>validation_flag</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the countrys largest payra mw thermal power pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the countrys largest payra mw thermal power pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the countrys largest payra mw thermal power pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the countrys largest payra mw thermal power pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the countrys largest payra mw thermal power pl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   original_text  ...                                               body\n",
              "0              1  ...  the countrys largest payra mw thermal power pl...\n",
              "1              1  ...  the countrys largest payra mw thermal power pl...\n",
              "2              1  ...  the countrys largest payra mw thermal power pl...\n",
              "3              1  ...  the countrys largest payra mw thermal power pl...\n",
              "4              1  ...  the countrys largest payra mw thermal power pl...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxuY8ejayJlc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "cc0468cb-ef40-4333-b785-5af1602940bb"
      },
      "source": [
        "val_set = pd.read_csv('gdrive/My Drive/SJ/data/val_set.tsv', sep='\\t', encoding='UTF-8', names=['original_text', 'validation_flag', 'body'])\n",
        "val_set.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>validation_flag</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>the under construction mw coal fired power pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>the under construction coal fired power plant ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>the under construction mw coal fired power pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>the under construction mw ember fired power em...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>the under construction mw coal fired power pla...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   original_text  ...                                               body\n",
              "0              0  ...  the under construction mw coal fired power pla...\n",
              "1              0  ...  the under construction coal fired power plant ...\n",
              "2              0  ...  the under construction mw coal fired power pla...\n",
              "3              0  ...  the under construction mw ember fired power em...\n",
              "4              0  ...  the under construction mw coal fired power pla...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOvXT5jwSrd1"
      },
      "source": [
        "# If the data is already splited into train_set and val_set\n",
        "def load_data(train_set, val_set, num_words, sequence_length, oov_token = None):\n",
        "    \n",
        "    # read articles of the trained set\n",
        "    train_articles = []\n",
        "    for text in train_set['body']:\n",
        "      text = text.strip()\n",
        "      train_articles.append(text)\n",
        "    \n",
        "    # read flags of the trained set\n",
        "    train_flags = []\n",
        "    for label in train_set['validation_flag']:\n",
        "      train_flags.append(label)\n",
        "\n",
        "    # read articles of the validation set\n",
        "    val_articles = []\n",
        "    for text in val_set['body']:\n",
        "      text = text.strip()\n",
        "      val_articles.append(text)\n",
        "    \n",
        "    #read articcles of the validation set\n",
        "    val_flags = []\n",
        "    for label in val_set['validation_flag']:\n",
        "      val_flags.append(label)\n",
        "\n",
        "    # tokenize the dataset corpus, delete uncommon words such as names, etc.\n",
        "    tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
        "    tokenizer.fit_on_texts(train_articles)\n",
        "    X_train = tokenizer.texts_to_sequences(train_articles)\n",
        "    X_train, y_train = np.array(X_train), np.array(train_flags)\n",
        "    # pad sequences with 0's\n",
        "    X_train = pad_sequences(X_train, maxlen=sequence_length)\n",
        "    # convert labels to one-hot encoded\n",
        "    y_train = to_categorical(y_train)\n",
        "\n",
        "    # transform your validation data the same way you transform your training data\n",
        "    # (since the data is already splitted into the train sets and validation sets)\n",
        "    tokenizer.fit_on_texts(val_articles)\n",
        "    X_val = tokenizer.texts_to_sequences(val_articles)\n",
        "    X_val, y_val = np.array(X_val), np.array(val_flags)\n",
        "    X_val = pad_sequences(X_val, maxlen=sequence_length)\n",
        "    y_val = to_categorical(y_val)\n",
        "\n",
        "    # # split data to training and testing sets\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 1)\n",
        "    \n",
        "    data = {}\n",
        "    data[\"X_train\"] = X_train\n",
        "    data[\"X_test\"]= X_val\n",
        "    data[\"y_train\"] = y_train\n",
        "    data[\"y_test\"] = y_val\n",
        "    data[\"tokenizer\"] = tokenizer\n",
        "    data[\"int2label\"] =  {0: \"negative\", 1: \"positive\"}\n",
        "    data[\"label2int\"] = {\"negative\": 0, \"positive\": 1}\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cto2sxJxA38x"
      },
      "source": [
        "# If the data is not splitted\n",
        "def load_data(dataFrame, num_words, test_size, sequence_length, oov_token = None):\n",
        "    \n",
        "    # read articles\n",
        "    articles = []\n",
        "    for text in dataFrame['body']:\n",
        "      text = text.strip()\n",
        "      articles.append(text)\n",
        "    \n",
        "    # read flags\n",
        "    flags = []\n",
        "    for label in dataFrame['validation_flag']:\n",
        "      flags.append(label)\n",
        "\n",
        "    # tokenize the dataset corpus, delete uncommon words such as names, etc.\n",
        "    tokenizer = Tokenizer(num_words = num_words, oov_token = oov_token)\n",
        "    tokenizer.fit_on_texts(articles)\n",
        "    X = tokenizer.texts_to_sequences(articles)\n",
        "    X, y = np.array(X), np.array(flags)\n",
        "\n",
        "    # pad sequences with 0's\n",
        "    X = pad_sequences(X, maxlen=sequence_length)\n",
        "\n",
        "    # convert labels to one-hot encoded\n",
        "    y  = to_categorical(y)\n",
        "\n",
        "    # split data to training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 1)\n",
        "    \n",
        "    data = {}\n",
        "    data[\"X_train\"] = X_train\n",
        "    data[\"X_test\"]= X_test\n",
        "    data[\"y_train\"] = y_train\n",
        "    data[\"y_test\"] = y_test\n",
        "    data[\"tokenizer\"] = tokenizer\n",
        "    data[\"int2label\"] =  {0: \"negative\", 1: \"positive\"}\n",
        "    data[\"label2int\"] = {\"negative\": 0, \"positive\": 1}\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sly-QkdlYrLF"
      },
      "source": [
        "# get pre-trained GloVe word vectors, which are pre-trained vectors that map each word to a vector of a specific size.\n",
        "# The size parameter is often called embedding size, although GloVe uses 50, 100, 200 or 300 embedding size vectors.\n",
        "\n",
        "def get_embedding_vectors(word_index, embedding_size=200):\n",
        "  embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n",
        "  with open(f\"gdrive/My Drive/SJ/glove.6B/glove.6B.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n",
        "    for line in tqdm(f, \"Reading GloVe\"):\n",
        "      values = line.split()\n",
        "      # get the word as the first word in the line\n",
        "      word = values[0]\n",
        "      if word in word_index:\n",
        "          idx = word_index[word]\n",
        "          # get the vectors as the remaining values in the line\n",
        "          embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n",
        "  return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQhuMS3NoDyc"
      },
      "source": [
        "def create_model(word_index, units=128, n_layers=1, cell=LSTM, bidirectional=False,\n",
        "                 embedding_size=100, sequence_length=100, dropout=0.3, \n",
        "                 loss=\"categorical_crossentropy\", optimizer=\"adam\", \n",
        "                 output_length=2):\n",
        "  # Constructs a RNN model given its parameters\n",
        "  embedding_matrix = get_embedding_vectors(word_index, embedding_size)\n",
        "  model = Sequential()\n",
        "  \n",
        "  # add the embedding layer\n",
        "  model.add(Embedding(len(word_index) + 1, embedding_size, weights=[embedding_matrix],\n",
        "                      trainable=False, input_length=sequence_length))\n",
        "  for i in range(n_layers):\n",
        "    if i == n_layers - 1:\n",
        "        # last layer\n",
        "        if bidirectional:\n",
        "            model.add(Bidirectional(cell(units, return_sequences=False)))\n",
        "        else:\n",
        "            model.add(cell(units, return_sequences=False))\n",
        "    else:\n",
        "        # first layer or hidden layers\n",
        "        if bidirectional:\n",
        "            model.add(Bidirectional(cell(units, return_sequences=True)))\n",
        "        else:\n",
        "            model.add(cell(units, return_sequences=True))\n",
        "    model.add(Dropout(dropout))\n",
        "  model.add(Dense(output_length, activation=\"softmax\"))\n",
        "  # compile the model\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmJKGSFVKDRy"
      },
      "source": [
        "# max number of words in each sentence\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# N-Dimensional GloVe embedding vectors\n",
        "EMBEDDING_SIZE = 300\n",
        "\n",
        "# number of words to use, discarding the rest\n",
        "N_WORDS = 10000\n",
        "\n",
        "# out of vocabulary token\n",
        "OOV_TOKEN = None\n",
        "\n",
        "# 30% testing set, 70% training set\n",
        "TEST_SIZE = 0.3\n",
        "\n",
        "# number of CELL layers\n",
        "N_LAYERS = 1\n",
        "\n",
        "# the RNN cell to use, LSTM in this case\n",
        "RNN_CELL = LSTM\n",
        "\n",
        "# whether it's a bidirectional RNN\n",
        "IS_BIDIRECTIONAL = True\n",
        "\n",
        "# number of units (RNN_CELL ,nodes) in each layer\n",
        "UNITS = 64\n",
        "\n",
        "# dropout rate\n",
        "DROPOUT = 0.3\n",
        "\n",
        "### Training parameters\n",
        "LOSS = \"categorical_crossentropy\"\n",
        "OPTIMIZER = \"adam\"\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 6\n",
        "\n",
        "def get_model_name(dataset_name):\n",
        "  # construct the unique model name\n",
        "  model_name = f\"{dataset_name}-{RNN_CELL.__name__}-seq-{SEQUENCE_LENGTH}-em-{EMBEDDING_SIZE}-w-{N_WORDS}-layers-{N_LAYERS}-units-{UNITS}-opt-{OPTIMIZER}-BS-{BATCH_SIZE}-d-{DROPOUT}\"\n",
        "  if IS_BIDIRECTIONAL:\n",
        "    # add 'bid' str if bidirectional\n",
        "    model_name = \"bid-\" + model_name\n",
        "  if OOV_TOKEN:\n",
        "    # add 'oov' str if OOV token is specified\n",
        "    model_name += \"-oov\"\n",
        "  return model_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cYUBu8WNRjQ"
      },
      "source": [
        "# create these folders if they does not exist\n",
        "main_dir = \"gdrive/My Drive/SJ/\"\n",
        "if not os.path.isdir(main_dir + \"results\"):\n",
        "  os.mkdir(main_dir + \"results\")\n",
        "if not os.path.isdir(main_dir + \"logs\"):\n",
        "  os.mkdir(main_dir + \"logs\")\n",
        "if not os.path.isdir(main_dir + \"data\"):\n",
        "  os.mkdir(main_dir + \"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1BirFKiKkfQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "bf71fd1d-552a-4dba-cb91-e5f2563c14c3"
      },
      "source": [
        "# # load the data - when the data is splitted\n",
        "# data = load_data(train_set, val_set, N_WORDS, SEQUENCE_LENGTH, oov_token=OOV_TOKEN)\n",
        "\n",
        "# load the data - when the data is not splitted\n",
        "data = load_data(dataFrame, N_WORDS, TEST_SIZE, SEQUENCE_LENGTH, oov_token=OOV_TOKEN)\n",
        "\n",
        "# construct the model\n",
        "model = create_model(data[\"tokenizer\"].word_index, units=UNITS, n_layers=N_LAYERS, \n",
        "                    cell=RNN_CELL, bidirectional=IS_BIDIRECTIONAL, embedding_size=EMBEDDING_SIZE, \n",
        "                    sequence_length=SEQUENCE_LENGTH, dropout=DROPOUT, \n",
        "                    loss=LOSS, optimizer=OPTIMIZER, output_length=data[\"y_train\"][0].shape[0])\n",
        "model.summary()\n",
        "\n",
        "model_name = get_model_name(\"policy_dataset\")\n",
        "\n",
        "# using tensorboard on 'logs' folder\n",
        "tensorboard = TensorBoard(log_dir=os.path.join(main_dir + \"logs\", model_name))\n",
        "\n",
        "# start training\n",
        "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
        "                    callbacks=[tensorboard],\n",
        "                    verbose=1)\n",
        "\n",
        "# save the resulting model into 'results' folder\n",
        "model.save(os.path.join(main_dir + \"results\", model_name) + \".h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading GloVe: 400000it [00:17, 23062.35it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 300)          5388600   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 5,575,738\n",
            "Trainable params: 187,138\n",
            "Non-trainable params: 5,388,600\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "2/3 [===================>..........] - ETA: 0s - loss: 0.7122 - accuracy: 0.4844WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.251716). Check your callbacks.\n",
            "3/3 [==============================] - 2s 556ms/step - loss: 0.7068 - accuracy: 0.4929 - val_loss: 0.6490 - val_accuracy: 0.6500\n",
            "Epoch 2/6\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.5804 - accuracy: 0.7286 - val_loss: 0.6011 - val_accuracy: 0.7833\n",
            "Epoch 3/6\n",
            "3/3 [==============================] - 0s 47ms/step - loss: 0.5153 - accuracy: 0.7714 - val_loss: 0.5684 - val_accuracy: 0.7167\n",
            "Epoch 4/6\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.4538 - accuracy: 0.8000 - val_loss: 0.5409 - val_accuracy: 0.7167\n",
            "Epoch 5/6\n",
            "3/3 [==============================] - 0s 46ms/step - loss: 0.3973 - accuracy: 0.8214 - val_loss: 0.5022 - val_accuracy: 0.7500\n",
            "Epoch 6/6\n",
            "3/3 [==============================] - 0s 50ms/step - loss: 0.3237 - accuracy: 0.8857 - val_loss: 0.4664 - val_accuracy: 0.7167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vihZP1Y8gjJk"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# saving the tokenizer\n",
        "with open('gdrive/My Drive/SJ/tocknizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(data['tokenizer'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkK8mShtLRj9"
      },
      "source": [
        "# load and evaluate a saved model\n",
        "from numpy import loadtxt\n",
        "import tensorflow as tf\n",
        " \n",
        "# load model\n",
        "model = tf.keras.models.load_model('gdrive/My Drive/SJ/results/bid-policy_dataset-LSTM-seq-300-em-300-w-10000-layers-1-units-64-opt-adam-BS-64-d-0.3.h5')\n",
        "\n",
        "# max number of words in each sentence\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "# N-Dimensional GloVe embedding vectors\n",
        "EMBEDDING_SIZE = 200\n",
        "\n",
        "# number of words to use, discarding the rest\n",
        "N_WORDS = 10000\n",
        "\n",
        "# out of vocabulary token\n",
        "OOV_TOKEN = None\n",
        "\n",
        "# 30% testing set, 70% training set\n",
        "TEST_SIZE = 0.3\n",
        "\n",
        "# load the data - when the data is not splitted\n",
        "data = load_data(dataFrame, N_WORDS, TEST_SIZE, SEQUENCE_LENGTH, oov_token=OOV_TOKEN)\n",
        "\n",
        "def get_predictions(text):\n",
        "  sequence = data[\"tokenizer\"].texts_to_sequences([text])\n",
        "  # pad the sequences\n",
        "  sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "  # get the prediction\n",
        "  prediction = model.predict(sequence)[0]\n",
        "  return prediction, data[\"int2label\"][np.argmax(prediction)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XQiq1VI0RR7"
      },
      "source": [
        "def get_predictions(text):\n",
        "  sequence = data[\"tokenizer\"].texts_to_sequences([text])\n",
        "  # pad the sequences\n",
        "  sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "  # get the prediction\n",
        "  prediction = model.predict(sequence)[0]\n",
        "  return prediction, data[\"int2label\"][np.argmax(prediction)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dUciCXdPM1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bb6585e4-14ed-45ab-80d7-121c88fc2f7f"
      },
      "source": [
        "text = \"Completion of the 150-km Tanakpur-Pithoragarh stretch of the chardham all-weather road project will be delayed by nearly six months due to the shortage of labourers amid the ongoing lockdown, an official said on Thursday. 'This particular stretch of the project was to be completed in June this year. However, it is not likely to be ready before November now as all migrant labourers working on the project have left since their exodus began with the announcement of the lockdown,' NHAI Executive Engineer L D Mathela who is in charge of the project said. The shortage of labourers and technical staff has affected cutting, hot mixing, crushing and quarrying work of the project, the National Highways Authority of India (NHAI) official said. Even if work on the project resumes after May 3 it is bound to be affected by the onset of monsoon in June-July, he further said. Frequent rain and landslides during monsoon often lead to suspension of work on the project slowing its progress, Mathela said. The chardham all-weather road is a pet project of Prime Minister Narendra Modi who keeps monitoring its progress personally.\"\n",
        "output_vector, prediction = get_predictions(text)\n",
        "print(\"Output vector:\", output_vector)\n",
        "print(\"Prediction:\", prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output vector: [0.4810909  0.51890904]\n",
            "Prediction: positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhaweLvPP6yH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "734a2ef6-66a5-4900-dcc1-abf06f0c805c"
      },
      "source": [
        "text = \"Looks like Taylor Swift and Ariana Grande were right to caution their fans about the coronavirus pandemic: Everyone who is able to stay home amid the spread of the COVID-19 virus should do so. Not even your favorite celebrities are immune. Since Tom Hanks and Rita Wilson confirmed their diagnoses in Australia, the question became a matter of when other famous faces would come down with the virus next, not if. Even Heidi Klum, who says she could not get a hold of a test, revealed she’d been experiencing symptoms. Here’s every celebrity who’s said they tested positive for coronavirus:\"\n",
        "output_vector, prediction = get_predictions(text)\n",
        "print(\"Output vector:\", output_vector)\n",
        "print(\"Prediction:\", prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output vector: [0.9787909  0.02120909]\n",
            "Prediction: negative\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iqFGCUX7thJ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}